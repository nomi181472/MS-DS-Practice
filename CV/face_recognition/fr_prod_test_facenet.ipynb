{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Description]: in the notebook facerecognization is testing with base classifiers using landmarks detected from dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\per\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the CNN architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN,InceptionResnetV1\n",
    "from facenet_pytorch.models.inception_resnet_v1 import BasicConv2d\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class FaceClassification(L.LightningModule):\n",
    "    def __init__(self,num_classes):\n",
    "        super().__init__()\n",
    "        self.gray_scale_input = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)  \n",
    "        inception =InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=num_classes)\n",
    "                # Change the input layer to accept grayscale images\n",
    "        \n",
    "       \n",
    "        self.classifier = inception.to(device)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        #print()\n",
    "        if x.ndim ==3:\n",
    "            x = x.unsqueeze(1)\n",
    "            x=self.gray_scale_input(x)\n",
    "       \n",
    "        x=self.classifier(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "# Save the model's state dictionary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier weight loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from custom_model.basic_landmark_classifier import LandmarkClassifier\n",
    "import PIL.Image\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import torch\n",
    "normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "\n",
    "class FRBaseFacenet:\n",
    "    def __init__(self,classifier_weight_path,id_name_csv_path,image_size=128) :\n",
    "        names_df=pd.read_csv(id_name_csv_path,index_col=0)\n",
    "        self.ids_name=names_df[[\"name\",\"id\"]].groupby([\"name\",\"id\"]).mean().reset_index()\n",
    "        self.classifier_weight_path=Path(classifier_weight_path)\n",
    "        self.image_size=image_size\n",
    "        self.load_pretrained_weight()\n",
    "\n",
    "\n",
    "    def get_class_name(self,pred_id):\n",
    "        name=self.ids_name[self.ids_name[\"id\"]==pred_id][\"name\"].values[-1]\n",
    "        return name.split(\"_\")[-1]\n",
    "    def load_pretrained_weight(self):\n",
    "        # If required, create a face detection pipeline using MTCNN:\n",
    "        self.mtcnn = MTCNN(image_size=self.image_size,keep_all=True, device=device)\n",
    "        try:\n",
    "            self.classifier =FaceClassification(len(self.ids_name))\n",
    "            self.classifier.load_state_dict(torch.load(str(self.classifier_weight_path)))\n",
    "            print(f\"classifier weight loaded successfully\")\n",
    "        except  Exception as e:\n",
    "            print(f\"classifier loading weight exception occur {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def get_gray_img(self, image):\n",
    "        # Check if the input is a valid numpy array\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # Check if the image is in RGB format\n",
    "            if image.ndim == 3 and image.shape[2] == 3:\n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                return gray_image\n",
    "            else:\n",
    "                raise ValueError(f\"Expected an RGB image, but got shape: {image.shape}\")\n",
    "        else:\n",
    "            raise TypeError(f\"Expected a numpy array, but got type: {type(image)}\") \n",
    "\n",
    "    def face_recognize(self, video_path,resize_width=460)->None:\n",
    "        cam = cv2.VideoCapture(str(video_path))\n",
    "        cv2.namedWindow(\"Capture Face\")\n",
    "        self.classifier.eval()\n",
    "        while True:\n",
    "            ret, frame = cam.read()\n",
    "            if not ret:\n",
    "                break                \n",
    "            # Resize the frame to the specified width while maintaining aspect ratio\n",
    "            height, width = frame.shape[:2]\n",
    "            aspect_ratio = height / width\n",
    "            new_height = int(resize_width * aspect_ratio)\n",
    "            frame_resized = cv2.resize(frame, (resize_width, new_height))\n",
    "            #mtcnn\n",
    "            boxes,_ = self.mtcnn.detect(frame_resized)\n",
    "            cropped_faces=[]\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box)  # Convert to int\n",
    "                    # Draw rectangle around the detected face\n",
    "                    \n",
    "                    face = frame_resized[y1:y2, x1:x2]\n",
    "                    if face.size == 0:\n",
    "                        print(\"zero size face\")\n",
    "                        continue\n",
    "                    cv2.rectangle(frame_resized, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    face_resized = cv2.resize(face, (self.image_size, self.image_size))\n",
    "                    \n",
    "                    face_resized=self.get_gray_img(face_resized)\n",
    "                    \n",
    "                    face_resized=torch.tensor(face_resized,dtype=torch.float32,device=device)\n",
    "                    \n",
    "                    # Add the channel dimension (1, H, W) for grayscale image\n",
    "                    if len(face_resized.shape) == 2:  # if it's grayscale (H, W)\n",
    "                        face_resized = face_resized.unsqueeze(0)  # Convert to (1, H, W) format\n",
    "                    face_resized/=255.0\n",
    "                    face_resized=normalize(face_resized)\n",
    "                    face_resized=face_resized.squeeze(0) \n",
    "                    cropped_faces.append(face_resized)\n",
    "                if len(cropped_faces)>0:\n",
    "\n",
    "                    cropped_faces=torch.tensor( np.stack(cropped_faces),dtype=torch.float32,device=device)\n",
    "                    \n",
    "                    logits = self.classifier(cropped_faces)\n",
    "                    probs=F.softmax(logits, dim=1)\n",
    "                    #print(probs)\n",
    "                    _, indices = torch.max(probs, dim=1)\n",
    "                    for i,at_prob in enumerate(probs):\n",
    "                        \n",
    "                        index=indices[i].item()\n",
    "                        prob=at_prob[index]\n",
    "                        #print(f\"prob:{prob}\")\n",
    "                        if prob>0.6:\n",
    "                            text_color = (0, 255, 0)\n",
    "                            cls_name=self.get_class_name(index)\n",
    "                            cv2.putText(frame_resized, cls_name, (x1 + 2, y1 - 2),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, text_color,1, cv2.LINE_AA)  # White text, thicker, anti-aliased\n",
    "                            \n",
    "                        elif prob<0.3:\n",
    "                            cls_name=\"not in database\"\n",
    "                            #text_color=(0, 0, 255)\n",
    "                            #cv2.putText(frame_resized, cls_name, (x1 + 2, y1 - 2),\n",
    "                            #        cv2.FONT_HERSHEY_SIMPLEX, 1, text_color,1, cv2.LINE_AA)  # White text, thicker, anti-aliased\n",
    "                            \n",
    "                        else:\n",
    "                            cls_name=\"recognizing...\"\n",
    "                            text_color = (255, 255, 255)\n",
    "                            cv2.putText(frame_resized, cls_name, (x1 + 2, y1 - 2),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, text_color,1, cv2.LINE_AA)  # White text, thicker, anti-aliased\n",
    "                            \n",
    "                        print(f\"cls_name:{cls_name} with prob{prob}\")\n",
    "                else:\n",
    "                    print(\"no faces\")\n",
    "\n",
    "\n",
    "                cv2.imshow(\"Capture Face\", frame_resized)\n",
    "                \n",
    "                k = cv2.waitKey(1)\n",
    "                if k % 256 == 27 :  # ESC or 20 images collected\n",
    "                    break\n",
    "\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    " \n",
    "        \n",
    "         \n",
    "        \n",
    "obj=FRBaseFacenet(\n",
    "\n",
    "\n",
    "    \"./weights/face_classification_model_3.pth\",\n",
    "    \"./weights/name_id.csv\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_name:sameer with prob0.9996067881584167\n",
      "cls_name:sameer with prob0.9996215105056763\n",
      "cls_name:sameer with prob0.999626874923706\n",
      "cls_name:sameer with prob0.9996069073677063\n",
      "cls_name:sameer with prob0.9995924830436707\n",
      "cls_name:sameer with prob0.9996188879013062\n",
      "cls_name:sameer with prob0.999596893787384\n",
      "cls_name:sameer with prob0.9995887875556946\n",
      "cls_name:sameer with prob0.9996086955070496\n",
      "cls_name:sameer with prob0.9996199607849121\n",
      "cls_name:sameer with prob0.9996022582054138\n",
      "cls_name:sameer with prob0.9995922446250916\n",
      "cls_name:sameer with prob0.9996131062507629\n",
      "cls_name:sameer with prob0.9996187686920166\n",
      "cls_name:sameer with prob0.9995974898338318\n",
      "cls_name:sameer with prob0.9995997548103333\n",
      "cls_name:sameer with prob0.9996180534362793\n",
      "cls_name:sameer with prob0.9996089339256287\n",
      "cls_name:sameer with prob0.9995989203453064\n",
      "cls_name:sameer with prob0.9996107220649719\n",
      "cls_name:sameer with prob0.9995982050895691\n",
      "cls_name:sameer with prob0.9996129870414734\n",
      "cls_name:sameer with prob0.9995964169502258\n",
      "cls_name:sameer with prob0.9996166229248047\n",
      "cls_name:sameer with prob0.9995966553688049\n",
      "cls_name:sameer with prob0.9995971322059631\n",
      "cls_name:sameer with prob0.9996139407157898\n",
      "cls_name:sameer with prob0.9995951056480408\n",
      "cls_name:sameer with prob0.9996151924133301\n",
      "cls_name:sameer with prob0.9995943903923035\n",
      "cls_name:sameer with prob0.9996224641799927\n",
      "cls_name:sameer with prob0.9995934367179871\n",
      "cls_name:sameer with prob0.999595582485199\n",
      "cls_name:sameer with prob0.9996219873428345\n",
      "cls_name:sameer with prob0.999599039554596\n",
      "cls_name:sameer with prob0.9996159076690674\n",
      "cls_name:sameer with prob0.9995957016944885\n",
      "cls_name:sameer with prob0.9996178150177002\n",
      "cls_name:sameer with prob0.9995890259742737\n",
      "cls_name:sameer with prob0.9996153116226196\n",
      "cls_name:sameer with prob0.9995941519737244\n",
      "cls_name:sameer with prob0.9996150732040405\n",
      "cls_name:sameer with prob0.9995957016944885\n",
      "cls_name:sameer with prob0.9996113181114197\n",
      "cls_name:sameer with prob0.9995920062065125\n",
      "cls_name:sameer with prob0.9996144771575928\n",
      "cls_name:sameer with prob0.9995906949043274\n",
      "cls_name:sameer with prob0.9996131062507629\n",
      "cls_name:sameer with prob0.999609649181366\n",
      "cls_name:sameer with prob0.9995917677879333\n",
      "cls_name:sameer with prob0.9996175765991211\n",
      "cls_name:sameer with prob0.9995934367179871\n",
      "cls_name:sameer with prob0.9996206760406494\n",
      "cls_name:sameer with prob0.9995928406715393\n",
      "cls_name:sameer with prob0.9996179342269897\n",
      "cls_name:sameer with prob0.9995809197425842\n",
      "cls_name:sameer with prob0.9995982050895691\n",
      "cls_name:sameer with prob0.9996123909950256\n",
      "cls_name:sameer with prob0.9995809197425842\n",
      "cls_name:sameer with prob0.9995900988578796\n",
      "cls_name:sameer with prob0.9996071457862854\n",
      "cls_name:sameer with prob0.9995947480201721\n",
      "cls_name:sameer with prob0.9996131062507629\n",
      "cls_name:sameer with prob0.999595582485199\n",
      "cls_name:sameer with prob0.9996102452278137\n",
      "cls_name:sameer with prob0.9995983242988586\n",
      "cls_name:sameer with prob0.9996098875999451\n",
      "cls_name:sameer with prob0.9995943903923035\n",
      "cls_name:sameer with prob0.9996147155761719\n",
      "cls_name:sameer with prob0.999592125415802\n",
      "cls_name:sameer with prob0.9996165037155151\n",
      "cls_name:sameer with prob0.9995903372764587\n",
      "cls_name:sameer with prob0.9996102452278137\n",
      "cls_name:sameer with prob0.9995927214622498\n",
      "cls_name:sameer with prob0.9996101260185242\n",
      "cls_name:sameer with prob0.9995878338813782\n",
      "cls_name:sameer with prob0.9995977282524109\n",
      "cls_name:sameer with prob0.9996092915534973\n",
      "cls_name:sameer with prob0.9995895028114319\n",
      "cls_name:sameer with prob0.999612033367157\n",
      "cls_name:sameer with prob0.9995920062065125\n",
      "cls_name:sameer with prob0.9996174573898315\n",
      "cls_name:sameer with prob0.999594509601593\n",
      "cls_name:sameer with prob0.9996180534362793\n",
      "cls_name:sameer with prob0.9995916485786438\n",
      "cls_name:sameer with prob0.999589741230011\n",
      "cls_name:sameer with prob0.9996172189712524\n",
      "cls_name:sameer with prob0.9996163845062256\n",
      "cls_name:sameer with prob0.9996021389961243\n",
      "cls_name:sameer with prob0.9995961785316467\n",
      "cls_name:sameer with prob0.999617338180542\n",
      "cls_name:sameer with prob0.9995954632759094\n",
      "cls_name:sameer with prob0.9996101260185242\n",
      "cls_name:sameer with prob0.9995967745780945\n",
      "cls_name:sameer with prob0.9996121525764465\n",
      "cls_name:sameer with prob0.9995980858802795\n",
      "cls_name:sameer with prob0.9996097683906555\n",
      "cls_name:sameer with prob0.9996180534362793\n",
      "cls_name:sameer with prob0.9995970129966736\n",
      "cls_name:sameer with prob0.9995974898338318\n",
      "cls_name:sameer with prob0.9996150732040405\n",
      "cls_name:sameer with prob0.9995993971824646\n",
      "cls_name:sameer with prob0.9996159076690674\n",
      "cls_name:sameer with prob0.9995959401130676\n",
      "cls_name:sameer with prob0.9996169805526733\n",
      "cls_name:sameer with prob0.9995936751365662\n",
      "cls_name:sameer with prob0.9996193647384644\n",
      "cls_name:sameer with prob0.9995989203453064\n",
      "cls_name:sameer with prob0.9996108412742615\n",
      "cls_name:sameer with prob0.9996021389961243\n",
      "cls_name:sameer with prob0.9996154308319092\n",
      "cls_name:sameer with prob0.9995970129966736\n",
      "cls_name:sameer with prob0.9996176958084106\n",
      "cls_name:sameer with prob0.9995920062065125\n",
      "cls_name:sameer with prob0.9996199607849121\n",
      "cls_name:sameer with prob0.9995974898338318\n",
      "cls_name:sameer with prob0.9996145963668823\n",
      "cls_name:sameer with prob0.9995985627174377\n",
      "cls_name:sameer with prob0.9996137022972107\n",
      "cls_name:sameer with prob0.9996019005775452\n",
      "cls_name:sameer with prob0.9996167421340942\n",
      "cls_name:sameer with prob0.9995991587638855\n",
      "cls_name:sameer with prob0.999617338180542\n",
      "cls_name:sameer with prob0.9995948672294617\n",
      "cls_name:sameer with prob0.9996151924133301\n",
      "cls_name:sameer with prob0.9995952248573303\n",
      "cls_name:sameer with prob0.9996141195297241\n",
      "cls_name:sameer with prob0.9996200799942017\n",
      "cls_name:sameer with prob0.9995946288108826\n",
      "cls_name:sameer with prob0.9995914101600647\n",
      "cls_name:sameer with prob0.9996142387390137\n",
      "cls_name:sameer with prob0.9995960593223572\n",
      "cls_name:sameer with prob0.9995962977409363\n",
      "cls_name:sameer with prob0.999614953994751\n",
      "cls_name:sameer with prob0.9996163845062256\n",
      "cls_name:sameer with prob0.9995965361595154\n",
      "cls_name:sameer with prob0.9996150732040405\n",
      "cls_name:sameer with prob0.99959796667099\n",
      "cls_name:sameer with prob0.9996159076690674\n",
      "cls_name:sameer with prob0.9995970129966736\n",
      "cls_name:sameer with prob0.9996167421340942\n",
      "cls_name:sameer with prob0.9995939135551453\n",
      "cls_name:sameer with prob0.9996194839477539\n",
      "cls_name:sameer with prob0.9995952248573303\n",
      "cls_name:sameer with prob0.9996145963668823\n",
      "cls_name:sameer with prob0.9995966553688049\n",
      "cls_name:sameer with prob0.9996150732040405\n",
      "cls_name:sameer with prob0.9996190071105957\n",
      "cls_name:sameer with prob0.9995965361595154\n",
      "cls_name:sameer with prob0.9996172189712524\n",
      "cls_name:sameer with prob0.9996153116226196\n",
      "cls_name:sameer with prob0.9995983242988586\n",
      "cls_name:sameer with prob0.9995943903923035\n",
      "cls_name:sameer with prob0.9996155500411987\n",
      "cls_name:sameer with prob0.9995978474617004\n",
      "cls_name:sameer with prob0.9996185302734375\n"
     ]
    }
   ],
   "source": [
    "obj.face_recognize(\"./videos/vid1.mp4\",600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "per",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
