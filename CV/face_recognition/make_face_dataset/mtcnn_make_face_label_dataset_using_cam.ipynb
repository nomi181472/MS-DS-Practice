{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7a9683-c3d1-4348-a9bd-20eade36e1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_set_not_for_upload\\23_saqib\n",
      "saved->23_saqib_1.jpg\n",
      "saved->23_saqib_2.jpg\n",
      "saved->23_saqib_3.jpg\n",
      "saved->23_saqib_4.jpg\n",
      "saved->23_saqib_5.jpg\n",
      "saved->23_saqib_6.jpg\n",
      "saved->23_saqib_7.jpg\n",
      "saved->23_saqib_8.jpg\n",
      "saved->23_saqib_9.jpg\n",
      "saved->23_saqib_10.jpg\n",
      "saved->23_saqib_11.jpg\n",
      "saved->23_saqib_12.jpg\n",
      "saved->23_saqib_13.jpg\n",
      "saved->23_saqib_14.jpg\n",
      "saved->23_saqib_15.jpg\n",
      "saved->23_saqib_16.jpg\n",
      "saved->23_saqib_17.jpg\n",
      "saved->23_saqib_18.jpg\n",
      "saved->23_saqib_19.jpg\n",
      "saved->23_saqib_20.jpg\n",
      "saved->23_saqib_21.jpg\n",
      "saved->23_saqib_22.jpg\n",
      "saved->23_saqib_23.jpg\n",
      "saved->23_saqib_24.jpg\n",
      "saved->23_saqib_25.jpg\n",
      "saved->23_saqib_26.jpg\n",
      "saved->23_saqib_27.jpg\n",
      "saved->23_saqib_28.jpg\n",
      "saved->23_saqib_29.jpg\n",
      "saved->23_saqib_30.jpg\n",
      "saved->23_saqib_31.jpg\n",
      "saved->23_saqib_32.jpg\n",
      "saved->23_saqib_33.jpg\n",
      "saved->23_saqib_34.jpg\n",
      "saved->23_saqib_35.jpg\n",
      "saved->23_saqib_36.jpg\n",
      "saved->23_saqib_37.jpg\n",
      "saved->23_saqib_38.jpg\n",
      "saved->23_saqib_39.jpg\n",
      "saved->23_saqib_40.jpg\n",
      "saved->23_saqib_41.jpg\n",
      "saved->23_saqib_42.jpg\n",
      "saved->23_saqib_43.jpg\n",
      "saved->23_saqib_44.jpg\n",
      "saved->23_saqib_45.jpg\n",
      "saved->23_saqib_46.jpg\n",
      "saved->23_saqib_47.jpg\n",
      "saved->23_saqib_48.jpg\n",
      "saved->23_saqib_49.jpg\n",
      "saved->23_saqib_50.jpg\n",
      "saved->23_saqib_51.jpg\n",
      "saved->23_saqib_52.jpg\n",
      "saved->23_saqib_53.jpg\n",
      "saved->23_saqib_54.jpg\n",
      "saved->23_saqib_55.jpg\n",
      "saved->23_saqib_56.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "def capture_images(cp, name, img_size=(128, 128), margin=20):\n",
    "    # Initialize MTCNN for face detection\n",
    "    mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow(\"Capture Face\")\n",
    "\n",
    "    face_id = name.split('/')[-1]  # Get the last part of the path for the face_id\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Detect faces\n",
    "        boxes, _ = mtcnn.detect(img_pil)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box)  # Convert float to int\n",
    "                \n",
    "                # Add margin\n",
    "                x1 = max(0, x1 - margin)\n",
    "                y1 = max(0, y1 - margin)\n",
    "                x2 = min(frame.shape[1], x2 + margin)\n",
    "                y2 = min(frame.shape[0], y2 + margin)\n",
    "\n",
    "                # Draw rectangle with margin\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                count += 1\n",
    "                \n",
    "                # Extract the face region\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                \n",
    "                # Convert to grayscale\n",
    "                gray_face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Resize the face to the desired size\n",
    "                resized_face = cv2.resize(gray_face, img_size)\n",
    "                \n",
    "                # Save the resized image\n",
    "                save_p = os.path.join(cp, f\"{name}_{count}.jpg\")\n",
    "                cv2.imwrite(save_p, resized_face)\n",
    "                print(f\"saved->{name}_{count}.jpg\")\n",
    "\n",
    "        cv2.imshow(\"Capture Face\", frame)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k % 256 == 27 :  # ESC or 20 images collected\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "data = \"data_set_not_for_upload\"\n",
    "if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "\n",
    "employee_name = \"23_saqib\"\n",
    "path = os.path.join(data, employee_name)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "print(path)\n",
    "capture_images(path, employee_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a85c5d-1cde-458d-a994-00106c637c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234ba3c-f375-4b99-8150-21fb36745290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
